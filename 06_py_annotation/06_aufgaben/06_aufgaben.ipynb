{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übungsaufgaben 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1 (eigenen Sentence-Segmenter erstellen)\n",
    "\n",
    "Satzsegmentierung (End-of-Sentence-Detection) kann als binäre Klassifikation verstanden werden (s. https://www.nltk.org/book/ch06.html#sentence-segmentation), die für jedes Token in einem Korpus entscheidet, ob es ein ***sentence boundary token*** ist oder nicht. Dies ist genauer eine **Sequenzklassifikation**, da die Entscheidung abhängt vom ***Kontext der Punktuationszeichen*** (z.B. `['Mr', '.']`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK besitzt mit `sent_tokenize` eine Methode zur Satzsegmentierung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You hear that Mr. Anderson? That is the sound of inevitability. Good-bye, Mr. Anderson! END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You hear that Mr. Anderson?', 'That is the sound of inevitability.', 'Good-bye, Mr. Anderson!', 'END']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sents = nltk.sent_tokenize(text)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Erzeugen Sie einen ***(1) regelbasierten*** sowie einen ***(2) auf Satz-Segmentationsdaten der Penn-Treebank trainierten*** **Punktuationsklassifikator zur Satzsegmentierung**. \n",
    "\n",
    "Input soll eine Wordliste mit einer einfachen Tokenisierung sein, wie in folgendem englischen Beispielsatz, mit dem Sie Ihre Klassifikatoren auch testen sollen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'hear', 'that', 'Mr', '.', 'Anderson', '?', 'That', 'is', 'the', 'sound', 'of', 'inevitability', '.', 'Good', '-', 'bye', ',', 'Mr', '.', 'Anderson', '!', 'END']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "test_words = re.findall(r'\\w+|[^\\w\\s]+', text)  #entspricht nltk.wordpunct_tokenize\n",
    "print(test_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Erklärung zum wordpunct_tokenize-REGEXP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'hear', 'that', 'Mr', 'Anderson', 'That', 'is', 'the', 'sound', 'of', 'inevitability', 'Good', 'bye', 'Mr', 'Anderson', 'END']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'\\w+', text)) #findet alle Wörter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '?', '.', '-', ',', '.', '!']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'[^\\w\\s]+', text)) #findet alle Punktuationszeichen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'hear', 'that', 'Mr.', 'Anderson?', 'That', 'is', 'the', 'sound', 'of', 'inevitability.', 'Good-bye,', 'Mr.', 'Anderson!', 'END']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'[^\\s]+', text)) #\\s matches whitespace characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a) Rule-based Sentence Segmentation\n",
    "\n",
    "\n",
    "Erstellen Sie einen einfachen regelbasierten Punctuation Tagger, der eine Liste von Wort- und Punktuationstokens in eine Liste von entsprechenden Satz-Tokenlisten auftrennt. Orientieren Sie sich dabei an https://en.wikipedia.org/wiki/Sentence_boundary_disambiguation :\n",
    "\n",
    ">   (a) If it's a period, it ends a sentence.<br>\n",
    "    (b) If the preceding token is in the hand-compiled list of abbreviations, then it doesn't end a sentence.<br>\n",
    "    (c) If the next token is capitalized, then it ends a sentence.        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_sentences_rule_based(words):\n",
    "    sent_list = []\n",
    "    sent = []\n",
    "    for index,token in enumerate(words):\n",
    "        sent.append(token)\n",
    "        if token in '.?!' and words[index-1] not in [\"Mr\", \"Mrs\", \"Ms\", \"Dr\"] and words[index+1][0].isupper():\n",
    "            sent_list.append(sent)\n",
    "            sent = []\n",
    "    return sent_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['You', 'hear', 'that', 'Mr', '.', 'Anderson', '?'],\n",
       " ['That', 'is', 'the', 'sound', 'of', 'inevitability', '.'],\n",
       " ['Good', '-', 'bye', ',', 'Mr', '.', 'Anderson', '!']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_sentences_rule_based(test_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b) Supervised Sentence Segmentation\n",
    "\n",
    "Trainieren Sie einen Punctuation Classifier mit Hilfe der Daten zur Satzsegmentierung der Penn-Treebank. Orientieren Sie sich dabei am Vorgehen in https://www.nltk.org/book/ch06.html#sentence-segmentation :\n",
    "\n",
    "- extract features for possible sentence-boundary tokens\n",
    "- learn mapping from feature-representations to binary end-of-sentence classes (boundary yes/no) \n",
    "- training data: corpus with annotation of sentence boundaries (e.g. treebanks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank # Sample of Penn Treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use Penn Treebank as Training Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first step is to obtain some data that has already been segmented into sentences \n",
    "#and convert it into a form that is suitable for extracting features:\n",
    "sents = nltk.corpus.treebank_raw.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.', 'START'], ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov', '.', '29', '.'], ['Mr', '.', 'Vinken', 'is', 'chairman', 'of', 'Elsevier', 'N', '.', 'V', '.,', 'the', 'Dutch', 'publishing', 'group', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(sents[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, tokens is a merged list of tokens from the individual sentences, \n",
    "#and boundaries is a set containing the indexes of all sentence-boundary tokens. \n",
    "\n",
    "tokens = []               # Initialize an empty list to store merged tokens\n",
    "boundaries = set()        # Initialize an empty set to store sentence boundary indexes\n",
    "offset = 0                # Initialize offset to keep track of the current position in the merged tokens list\n",
    "\n",
    "for sent in sents:        # Iterate over each sentence in the list of sentences\n",
    "    tokens.extend(sent)   # Add all tokens from the current sentence to the merged tokens list\n",
    "    offset += len(sent)   # Update the offset by adding the length of the current sentence\n",
    "    boundaries.add(offset-1)  # Add the index of the last token of the current sentence to the boundaries set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '.'),\n",
       " (1, 'START'),\n",
       " (2, 'Pierre'),\n",
       " (3, 'Vinken'),\n",
       " (4, ','),\n",
       " (5, '61'),\n",
       " (6, 'years'),\n",
       " (7, 'old'),\n",
       " (8, ','),\n",
       " (9, 'will'),\n",
       " (10, 'join'),\n",
       " (11, 'the'),\n",
       " (12, 'board'),\n",
       " (13, 'as'),\n",
       " (14, 'a'),\n",
       " (15, 'nonexecutive'),\n",
       " (16, 'director'),\n",
       " (17, 'Nov'),\n",
       " (18, '.'),\n",
       " (19, '29'),\n",
       " (20, '.'),\n",
       " (21, 'Mr'),\n",
       " (22, '.'),\n",
       " (23, 'Vinken'),\n",
       " (24, 'is'),\n",
       " (25, 'chairman'),\n",
       " (26, 'of'),\n",
       " (27, 'Elsevier'),\n",
       " (28, 'N'),\n",
       " (29, '.'),\n",
       " (30, 'V'),\n",
       " (31, '.,'),\n",
       " (32, 'the'),\n",
       " (33, 'Dutch'),\n",
       " (34, 'publishing'),\n",
       " (35, 'group'),\n",
       " (36, '.'),\n",
       " (37, '.'),\n",
       " (38, 'START'),\n",
       " (39, 'Rudolph')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(index, token) for index,token in enumerate(tokens[0:40])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 20, 36, 38]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(boundaries))[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Feature-Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we need to specify the features of the data that will be used \n",
    "#in order to decide whether punctuation indicates a sentence-boundary:\n",
    "def punct_features(tokens, i):\n",
    "    return {'next-word-capitalized': tokens[i+1][0].isupper(),\n",
    "            'prev-word': tokens[i-1].lower(),\n",
    "            'punct': tokens[i],\n",
    "            'prev-word-is-one-char': len(tokens[i-1]) == 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on this feature extractor, we can create a list of labeled featuresets \n",
    "#by selecting all the punctuation tokens, and tagging whether they are boundary tokens or not:\n",
    "featuresets = [(punct_features(tokens, i), (i in boundaries))\n",
    "    for i in range(1, len(tokens)-1)\n",
    "    if tokens[i] in '.?!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'next-word-capitalized': False,\n",
       "   'prev-word': 'nov',\n",
       "   'punct': '.',\n",
       "   'prev-word-is-one-char': False},\n",
       "  False),\n",
       " ({'next-word-capitalized': True,\n",
       "   'prev-word': '29',\n",
       "   'punct': '.',\n",
       "   'prev-word-is-one-char': False},\n",
       "  True),\n",
       " ({'next-word-capitalized': True,\n",
       "   'prev-word': 'mr',\n",
       "   'punct': '.',\n",
       "   'prev-word-is-one-char': False},\n",
       "  False),\n",
       " ({'next-word-capitalized': True,\n",
       "   'prev-word': 'n',\n",
       "   'punct': '.',\n",
       "   'prev-word-is-one-char': True},\n",
       "  False),\n",
       " ({'next-word-capitalized': False,\n",
       "   'prev-word': 'group',\n",
       "   'punct': '.',\n",
       "   'prev-word-is-one-char': False},\n",
       "  True)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Train Simple Probabilistic Classifier (Naive Bayes):\n",
    "\n",
    "https://en.wikipedia.org/wiki/Naive_Bayes_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9461279461279462"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using these featuresets, we can train and evaluate a punctuation classifier:\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK: code_classification_based_segmenter\n",
    "\n",
    "def segment_sentences(words):\n",
    "    start = 0\n",
    "    sents = []\n",
    "    for i, word in enumerate(words):\n",
    "        if word in '.?!' and classifier.classify(punct_features(words, i)) == True:\n",
    "            sents.append(words[start:i+1])\n",
    "            start = i+1\n",
    "    if start < len(words):\n",
    "        sents.append(words[start:])\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'hear', 'that', 'Mr', '.', 'Anderson', '?', 'That', 'is', 'the', 'sound', 'of', 'inevitability', '.', 'Good', '-', 'bye', ',', 'Mr', '.', 'Anderson', '!', 'END']\n"
     ]
    }
   ],
   "source": [
    "print(test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['You', 'hear', 'that', 'Mr', '.', 'Anderson', '?'],\n",
       " ['That', 'is', 'the', 'sound', 'of', 'inevitability', '.'],\n",
       " ['Good', '-', 'bye', ',', 'Mr', '.', 'Anderson', '!'],\n",
       " ['END']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_sentences(test_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2 (Korpusannotation mit stanza)\n",
    "\n",
    "Annotieren Sie den Text in `wahlverwandschaften.txt` nach morphologischen, syntaktischen und semantischen Kategorien mit Hilfe der deutschen stanza-Modelle.\n",
    "\n",
    "Verwenden Sie dabei auch die CoNLL-Utilities von stanza für eine Transformation eines Dependency-analysierten Satzes in das CoNLL-Format, um es als NLTK-Dependency-Tree-Objekt einzulesen und zu plotten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "#stanza.download('de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### morphologische Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 11:41:28 INFO: Loading these models for language: de (German):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "=======================\n",
      "\n",
      "2023-06-13 11:41:28 INFO: Using device: cpu\n",
      "2023-06-13 11:41:28 INFO: Loading: tokenize\n",
      "2023-06-13 11:41:28 INFO: Loading: mwt\n",
      "2023-06-13 11:41:28 INFO: Loading: pos\n",
      "2023-06-13 11:41:28 INFO: Loading: lemma\n",
      "2023-06-13 11:41:28 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "text = open('../wahlverwandschaften.txt').read()\n",
    "nlp = stanza.Pipeline(lang='de', processors='tokenize, mwt, lemma, pos', download_method=None)\n",
    "#doc = nlp(text)#[0:1000])\n",
    "doc = nlp(text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\n",
       "   {\n",
       "     \"id\": 1,\n",
       "     \"text\": \"Die\",\n",
       "     \"lemma\": \"der\",\n",
       "     \"upos\": \"DET\",\n",
       "     \"xpos\": \"ART\",\n",
       "     \"feats\": \"Case=Nom|Definite=Def|Gender=Fem|Number=Plur|PronType=Art\",\n",
       "     \"start_char\": 0,\n",
       "     \"end_char\": 3\n",
       "   },\n",
       "   {\n",
       "     \"id\": 2,\n",
       "     \"text\": \"Wahlverwandtschaften\",\n",
       "     \"lemma\": \"Wahlverwandtschaft\",\n",
       "     \"upos\": \"NOUN\",\n",
       "     \"xpos\": \"NN\",\n",
       "     \"feats\": \"Case=Nom|Gender=Fem|Number=Plur\",\n",
       "     \"start_char\": 4,\n",
       "     \"end_char\": 24\n",
       "   }\n",
       " ]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sentences[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sein Geschäft war eben vollendet ;\n"
     ]
    }
   ],
   "source": [
    "print(*[f'{word.text}' for sent in doc.sentences[6:7] for word in sent.words], sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Sein \tlemma: sein\tupos: DET\txpos: PPOSAT\tfeats: Case=Nom|Gender=Neut|Gender[psor]=Masc,Neut|Number=Sing|Number[psor]=Sing|Person=3|Poss=Yes|PronType=Prs\n",
      "word: Geschäft \tlemma: Geschäft\tupos: NOUN\txpos: NN\tfeats: Case=Nom|Gender=Neut|Number=Sing\n",
      "word: war \tlemma: sein\tupos: AUX\txpos: VAFIN\tfeats: Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\n",
      "word: eben \tlemma: eben\tupos: ADV\txpos: ADV\tfeats: _\n",
      "word: vollendet \tlemma: vollenden\tupos: ADJ\txpos: VVPP\tfeats: Degree=Pos|VerbForm=Part\n",
      "word: ; \tlemma: ;\tupos: PUNCT\txpos: $.\tfeats: _\n"
     ]
    }
   ],
   "source": [
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}' for sent in doc.sentences[6:7] for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 11:41:33 WARNING: Language de package default expects mwt, which has been added\n",
      "2023-06-13 11:41:33 INFO: Loading these models for language: de (German):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| sentiment | sb10k   |\n",
      "=======================\n",
      "\n",
      "2023-06-13 11:41:33 INFO: Using device: cpu\n",
      "2023-06-13 11:41:33 INFO: Loading: tokenize\n",
      "2023-06-13 11:41:33 INFO: Loading: mwt\n",
      "2023-06-13 11:41:33 INFO: Loading: sentiment\n",
      "2023-06-13 11:41:34 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> sentence 0 : sentiment: 2\n",
      "Die Wahlverwandtschaften\n",
      "=> sentence 1 : sentiment: 2\n",
      "Ein Roman\n",
      "=> sentence 2 : sentiment: 2\n",
      "von Johann Wolfgang von Goethe\n",
      "=> sentence 3 : sentiment: 2\n",
      "Erster Teil\n",
      "=> sentence 4 : sentiment: 2\n",
      "Erstes Kapitel\n",
      "=> sentence 5 : sentiment: 1\n",
      "Eduard—so nennen wir einen reichen Baron in dem besten Mannesalter —Eduard hatte in seiner Baumschule die schönste Stunde eines Aprilnachmittags zugebracht , um frisch erhaltene Pfropfreiser auf junge Stämme zu bringen .\n",
      "=> sentence 6 : sentiment: 2\n",
      "Sein Geschäft war eben vollendet ;\n",
      "=> sentence 7 : sentiment: 1\n",
      "er legte die Gerätschaften in das Futteral zusammen und betrachtete seine Arbeit mit Vergnügen , als der Gärtner hinzutrat und sich an dem teilnehmenden Fleiße des Herrn ergetzte .\n",
      "=> sentence 8 : sentiment: 0\n",
      "„ Hast du meine Frau nicht gesehen ? “ fragte Eduard , indem er sich weiterzugehen anschickte .\n",
      "=> sentence 9 : sentiment: 1\n",
      "„ Drüben in den neuen Anlagen “ , versetzte der Gärtner .\n",
      "=> sentence 10 : sentiment: 1\n",
      "„ Die Mooshütte wird heute fertig , die sie an der Felswand , dem Schlosse gegenüber , gebaut hat .\n",
      "=> sentence 11 : sentiment: 1\n",
      "Alles ist recht schön geworden und muß Euer Gnaden gefallen .\n",
      "=> sentence 12 : sentiment: 1\n",
      "Man hat einen vortrefflichen Anblick : unten das Dorf , ein wenig rechter Hand die Kirche , über deren Turmspitze man fast hinwegsieht , gegenüber das Schloß und die Gär\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='de', processors='tokenize,sentiment', download_method=None)\n",
    "doc = nlp(text[0:1000])\n",
    "\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print('=> sentence',i, ': sentiment:', sentence.sentiment)\n",
    "    print(*[f'{word.text}' for word in sentence.words], sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 11:41:40 WARNING: Language de package default expects mwt, which has been added\n",
      "2023-06-13 11:41:40 INFO: Loading these models for language: de (German):\n",
      "============================\n",
      "| Processor | Package      |\n",
      "----------------------------\n",
      "| tokenize  | gsd          |\n",
      "| mwt       | gsd          |\n",
      "| ner       | germeval2014 |\n",
      "============================\n",
      "\n",
      "2023-06-13 11:41:40 INFO: Using device: cpu\n",
      "2023-06-13 11:41:40 INFO: Loading: tokenize\n",
      "2023-06-13 11:41:40 INFO: Loading: mwt\n",
      "2023-06-13 11:41:40 INFO: Loading: ner\n",
      "2023-06-13 11:41:42 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity: Johann Wolfgang von Goethe\ttype: PER\n",
      "entity: Eduard—so\ttype: PER\n",
      "entity: —Eduard\ttype: PER\n",
      "entity: Eduard\ttype: PER\n",
      "entity: Mooshütte\ttype: LOC\n",
      "entity: Eduard\ttype: PER\n",
      "entity: Eduard\ttype: PER\n",
      "entity: Eduard\ttype: PER\n",
      "entity: Mooshütte\ttype: LOC\n",
      "entity: Charlotte\ttype: PER\n",
      "entity: de\ttype: PER\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='de', processors='tokenize,ner', download_method=None)\n",
    "doc = nlp(text[0:2500])\n",
    "\n",
    "# sentence based NER output\n",
    "print(*[f'entity: {ent.text}\\ttype: {ent.type}' for ent in doc.ents], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "##### Inklusive Transformation in CONLL-Format für Plotting mit NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 11:41:51 WARNING: Language de package default expects mwt, which has been added\n",
      "2023-06-13 11:41:51 INFO: Loading these models for language: de (German):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "=======================\n",
      "\n",
      "2023-06-13 11:41:51 INFO: Using device: cpu\n",
      "2023-06-13 11:41:51 INFO: Loading: tokenize\n",
      "2023-06-13 11:41:51 INFO: Loading: mwt\n",
      "2023-06-13 11:41:51 INFO: Loading: pos\n",
      "2023-06-13 11:41:51 INFO: Loading: lemma\n",
      "2023-06-13 11:41:51 INFO: Loading: depparse\n",
      "2023-06-13 11:41:52 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='de', processors='tokenize,pos,lemma,depparse', download_method=None)\n",
    "doc = nlp(text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\n",
       "   {\n",
       "     \"id\": 1,\n",
       "     \"text\": \"Die\",\n",
       "     \"lemma\": \"der\",\n",
       "     \"upos\": \"DET\",\n",
       "     \"xpos\": \"ART\",\n",
       "     \"feats\": \"Case=Nom|Definite=Def|Gender=Fem|Number=Plur|PronType=Art\",\n",
       "     \"head\": 2,\n",
       "     \"deprel\": \"det\",\n",
       "     \"start_char\": 0,\n",
       "     \"end_char\": 3\n",
       "   },\n",
       "   {\n",
       "     \"id\": 2,\n",
       "     \"text\": \"Wahlverwandtschaften\",\n",
       "     \"lemma\": \"Wahlverwandtschaft\",\n",
       "     \"upos\": \"NOUN\",\n",
       "     \"xpos\": \"NN\",\n",
       "     \"feats\": \"Case=Nom|Gender=Fem|Number=Plur\",\n",
       "     \"head\": 0,\n",
       "     \"deprel\": \"root\",\n",
       "     \"start_char\": 4,\n",
       "     \"end_char\": 24\n",
       "   }\n",
       " ]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sentences[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1\tword: „\thead id: 7\thead: gesehen\tdeprel: punct\n",
      "id: 2\tword: Hast\thead id: 7\thead: gesehen\tdeprel: aux\n",
      "id: 3\tword: du\thead id: 7\thead: gesehen\tdeprel: nsubj\n",
      "id: 4\tword: meine\thead id: 5\thead: Frau\tdeprel: det:poss\n",
      "id: 5\tword: Frau\thead id: 7\thead: gesehen\tdeprel: obj\n",
      "id: 6\tword: nicht\thead id: 7\thead: gesehen\tdeprel: advmod\n",
      "id: 7\tword: gesehen\thead id: 10\thead: fragte\tdeprel: ccomp\n",
      "id: 8\tword: ?\thead id: 7\thead: gesehen\tdeprel: punct\n",
      "id: 9\tword: “\thead id: 7\thead: gesehen\tdeprel: punct\n",
      "id: 10\tword: fragte\thead id: 0\thead: root\tdeprel: root\n",
      "id: 11\tword: Eduard\thead id: 10\thead: fragte\tdeprel: nsubj\n",
      "id: 12\tword: ,\thead id: 17\thead: anschickte\tdeprel: punct\n",
      "id: 13\tword: indem\thead id: 17\thead: anschickte\tdeprel: mark\n",
      "id: 14\tword: er\thead id: 17\thead: anschickte\tdeprel: nsubj\n",
      "id: 15\tword: sich\thead id: 16\thead: weiterzugehen\tdeprel: obj\n",
      "id: 16\tword: weiterzugehen\thead id: 17\thead: anschickte\tdeprel: xcomp\n",
      "id: 17\tword: anschickte\thead id: 10\thead: fragte\tdeprel: advcl\n",
      "id: 18\tword: .\thead id: 10\thead: fragte\tdeprel: punct\n"
     ]
    }
   ],
   "source": [
    "print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences[8:9] for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1', '„', '\"', 'PUNCT', '$(', '_', '7', 'punct', '_', 'start_char=533|end_char=534'], ['2', 'Hast', 'haben', 'AUX', 'VAFIN', 'Mood=Ind|Number=Sing|Person=2|Tense=Pres|VerbForm=Fin', '7', 'aux', '_', 'start_char=534|end_char=538'], ['3', 'du', 'du', 'PRON', 'PPER', 'Case=Nom|Number=Sing|Person=2|PronType=Prs', '7', 'nsubj', '_', 'start_char=539|end_char=541'], ['4', 'meine', 'mein', 'DET', 'PPOSAT', 'Case=Acc|Gender=Fem|Number=Sing|Number[psor]=Sing|Person=1|Poss=Yes|PronType=Prs', '5', 'det:poss', '_', 'start_char=542|end_char=547'], ['5', 'Frau', 'Frau', 'NOUN', 'NN', 'Case=Acc|Gender=Fem|Number=Sing', '7', 'obj', '_', 'start_char=548|end_char=552'], ['6', 'nicht', 'nicht', 'PART', 'PTKNEG', 'Polarity=Neg', '7', 'advmod', '_', 'start_char=553|end_char=558'], ['7', 'gesehen', 'sehen', 'VERB', 'VVPP', 'VerbForm=Part', '10', 'ccomp', '_', 'start_char=559|end_char=566'], ['8', '?', '?', 'PUNCT', '$.', '_', '7', 'punct', '_', 'start_char=566|end_char=567'], ['9', '“', '\"', 'PUNCT', '$(', '_', '7', 'punct', '_', 'start_char=567|end_char=568'], ['10', 'fragte', 'fragen', 'VERB', 'VVFIN', 'Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin', '0', 'root', '_', 'start_char=569|end_char=575'], ['11', 'Eduard', 'Eduard', 'PROPN', 'NE', 'Case=Nom|Gender=Masc|Number=Sing', '10', 'nsubj', '_', 'start_char=576|end_char=582'], ['12', ',', ',', 'PUNCT', '$,', '_', '17', 'punct', '_', 'start_char=582|end_char=583'], ['13', 'indem', 'indem', 'SCONJ', 'KOUS', '_', '17', 'mark', '_', 'start_char=584|end_char=589'], ['14', 'er', 'er', 'PRON', 'PPER', 'Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs', '17', 'nsubj', '_', 'start_char=590|end_char=592'], ['15', 'sich', 'er|es|sie', 'PRON', 'PRF', 'Case=Acc|Number=Sing|Person=3|PronType=Prs|Reflex=Yes', '16', 'obj', '_', 'start_char=593|end_char=597'], ['16', 'weiterzugehen', 'weiterzugehen', 'VERB', 'VVIZU', 'VerbForm=Inf', '17', 'xcomp', '_', 'start_char=598|end_char=611'], ['17', 'anschickte', 'anschicken', 'VERB', 'VVFIN', 'Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin', '10', 'advcl', '_', 'start_char=612|end_char=622'], ['18', '.', '.', 'PUNCT', '$.', '_', '10', 'punct', '_', 'start_char=622|end_char=623']]\n"
     ]
    }
   ],
   "source": [
    "dicts = doc.to_dict() # dicts is List[List[Dict]], representing each token / word in each sentence in the document\n",
    "#dicts[0]\n",
    "\n",
    "from stanza.utils.conll import CoNLL\n",
    "conll = CoNLL.convert_dict(dicts) # conll is List[List[List]], representing each token / word in each sentence in the document\n",
    "print(conll[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t„\t\"\tPUNCT\t$(\t_\t7\tpunct\t_\tstart_char=533|end_char=534\n",
      "2\tHast\thaben\tAUX\tVAFIN\tMood=Ind|Number=Sing|Person=2|Tense=Pres|VerbForm=Fin\t7\taux\t_\tstart_char=534|end_char=538\n",
      "3\tdu\tdu\tPRON\tPPER\tCase=Nom|Number=Sing|Person=2|PronType=Prs\t7\tnsubj\t_\tstart_char=539|end_char=541\n",
      "4\tmeine\tmein\tDET\tPPOSAT\tCase=Acc|Gender=Fem|Number=Sing|Number[psor]=Sing|Person=1|Poss=Yes|PronType=Prs\t5\tdet:poss\t_\tstart_char=542|end_char=547\n",
      "5\tFrau\tFrau\tNOUN\tNN\tCase=Acc|Gender=Fem|Number=Sing\t7\tobj\t_\tstart_char=548|end_char=552\n",
      "6\tnicht\tnicht\tPART\tPTKNEG\tPolarity=Neg\t7\tadvmod\t_\tstart_char=553|end_char=558\n",
      "7\tgesehen\tsehen\tVERB\tVVPP\tVerbForm=Part\t10\tccomp\t_\tstart_char=559|end_char=566\n",
      "8\t?\t?\tPUNCT\t$.\t_\t7\tpunct\t_\tstart_char=566|end_char=567\n",
      "9\t“\t\"\tPUNCT\t$(\t_\t7\tpunct\t_\tstart_char=567|end_char=568\n",
      "10\tfragte\tfragen\tVERB\tVVFIN\tMood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\t0\troot\t_\tstart_char=569|end_char=575\n",
      "11\tEduard\tEduard\tPROPN\tNE\tCase=Nom|Gender=Masc|Number=Sing\t10\tnsubj\t_\tstart_char=576|end_char=582\n",
      "12\t,\t,\tPUNCT\t$,\t_\t17\tpunct\t_\tstart_char=582|end_char=583\n",
      "13\tindem\tindem\tSCONJ\tKOUS\t_\t17\tmark\t_\tstart_char=584|end_char=589\n",
      "14\ter\ter\tPRON\tPPER\tCase=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs\t17\tnsubj\t_\tstart_char=590|end_char=592\n",
      "15\tsich\ter|es|sie\tPRON\tPRF\tCase=Acc|Number=Sing|Person=3|PronType=Prs|Reflex=Yes\t16\tobj\t_\tstart_char=593|end_char=597\n",
      "16\tweiterzugehen\tweiterzugehen\tVERB\tVVIZU\tVerbForm=Inf\t17\txcomp\t_\tstart_char=598|end_char=611\n",
      "17\tanschickte\tanschicken\tVERB\tVVFIN\tMood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\t10\tadvcl\t_\tstart_char=612|end_char=622\n",
      "18\t.\t.\tPUNCT\t$.\t_\t10\tpunct\t_\tstart_char=622|end_char=623\n"
     ]
    }
   ],
   "source": [
    "tree_string = '\\n'.join(['\\t'.join(x) for x in conll[8]])\n",
    "print(tree_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"986pt\" height=\"388pt\"\n",
       " viewBox=\"0.00 0.00 986.22 388.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 384)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-384 982.2227,-384 982.2227,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"628\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"628\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">10 (fragte)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;10 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M628,-343.7616C628,-332.3597 628,-317.4342 628,-304.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"631.5001,-304.2121 628,-294.2121 624.5001,-304.2121 631.5001,-304.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"639.2759\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"316\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">7 (gesehen)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;7 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>10&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M590.0546,-265.5407C533.943,-250.074 428.7584,-221.0808 366.2866,-203.861\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"367.2116,-200.4855 356.641,-201.2023 365.3514,-207.2339 367.2116,-200.4855\"/>\n",
       "<text text-anchor=\"middle\" x=\"509.6587\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ccomp</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<text text-anchor=\"middle\" x=\"573\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">11 (Eduard)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>10&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M615.4605,-257.7903C611.6213,-252.1253 607.4209,-245.8348 603.6621,-240 598.9317,-232.6568 593.9286,-224.6208 589.3668,-217.1807\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"592.0703,-214.891 583.8783,-208.1723 586.0925,-218.5332 592.0703,-214.891\"/>\n",
       "<text text-anchor=\"middle\" x=\"619.1689\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>17</title>\n",
       "<text text-anchor=\"middle\" x=\"683\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">17 (anschickte)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;17 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>10&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M639.6641,-257.7616C647.2477,-245.9036 657.2686,-230.2345 665.7639,-216.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"668.9135,-218.5223 671.3527,-208.2121 663.0163,-214.7509 668.9135,-218.5223\"/>\n",
       "<text text-anchor=\"middle\" x=\"674.1587\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advcl</text>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>18</title>\n",
       "<text text-anchor=\"middle\" x=\"779\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">18 (.)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;18 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>10&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M661.1416,-257.8187C671.3808,-252.1548 682.6785,-245.8578 693,-240 709.5738,-230.5937 727.8377,-220.0068 743.2555,-211.0047\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"745.1047,-213.9779 751.9701,-205.9084 741.5709,-207.9353 745.1047,-213.9779\"/>\n",
       "<text text-anchor=\"middle\" x=\"732.5518\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">punct</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 („)</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"103\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (Hast)</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"179\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3 (du)</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"255\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (meine)</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"255\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">5 (Frau)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;4 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>5&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M255,-85.7616C255,-74.3597 255,-59.4342 255,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"258.5001,-46.2121 255,-36.2121 251.5001,-46.2121 258.5001,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"277.9448\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det:poss</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"336\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">6 (nicht)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;1 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>7&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M275.3349,-184.0909C239.063,-178.2663 184.7608,-168.1762 138.8965,-154 106.654,-144.0342 97.6192,-138.2543 63.3279,-122.0936\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"64.5775,-118.8144 54.036,-117.7525 61.6145,-125.1564 64.5775,-118.8144\"/>\n",
       "<text text-anchor=\"middle\" x=\"154.5518\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">punct</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;2 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>7&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M275.3486,-178.3751C254.0681,-171.9063 227.7344,-163.2946 204.7861,-154 184.0238,-145.5908 161.6111,-134.7799 143.1161,-125.3633\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"144.5978,-122.1896 134.1043,-120.7281 141.3961,-128.4145 144.5978,-122.1896\"/>\n",
       "<text text-anchor=\"middle\" x=\"215.1069\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">aux</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;3 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>7&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M279.2764,-171.8987C268.8833,-166.4434 257.6782,-160.2396 247.6621,-154 235.0725,-146.1572 221.7761,-136.7585 210.2718,-128.2372\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"212.3482,-125.4196 202.2473,-122.2223 208.1497,-131.0208 212.3482,-125.4196\"/>\n",
       "<text text-anchor=\"middle\" x=\"263.1689\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;5 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M303.0635,-171.7616C294.5717,-159.7896 283.3244,-143.9328 273.8451,-130.5685\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"276.5581,-128.3437 267.9179,-122.2121 270.8486,-132.3935 276.5581,-128.3437\"/>\n",
       "<text text-anchor=\"middle\" x=\"298.9448\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">obj</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;6 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>7&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M320.2415,-171.7616C322.9196,-160.2456 326.4336,-145.1353 329.4636,-132.1064\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"332.9084,-132.745 331.7646,-122.2121 326.0904,-131.1594 332.9084,-132.745\"/>\n",
       "<text text-anchor=\"middle\" x=\"350.5518\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"413\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">8 (?)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;8 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>7&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M352.2317,-171.9467C360.8363,-166.7861 369.6442,-160.716 377,-154 384.4253,-147.2205 391.2514,-138.6784 396.9172,-130.5821\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"399.9522,-132.3392 402.5833,-122.0763 394.1264,-128.4584 399.9522,-132.3392\"/>\n",
       "<text text-anchor=\"middle\" x=\"405.5518\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">punct</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"485\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">9 (“)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;9 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>7&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M356.5976,-180.5091C377.7808,-174.6035 403.6564,-165.8511 425,-154 437.1603,-147.248 449.2088,-137.8791 459.2828,-129.0746\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"461.8256,-131.4943 466.903,-122.1953 457.1349,-126.2984 461.8256,-131.4943\"/>\n",
       "<text text-anchor=\"middle\" x=\"460.5518\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">punct</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"557\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">12 (,)</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;12 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>17&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M652.8497,-171.9529C643.7685,-166.3481 633.8411,-160.0503 624.8965,-154 612.915,-145.8955 600.0673,-136.5982 588.8071,-128.2363\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"590.6512,-125.2449 580.5465,-122.0595 586.4593,-130.851 590.6512,-125.2449\"/>\n",
       "<text text-anchor=\"middle\" x=\"640.5518\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">punct</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>13</title>\n",
       "<text text-anchor=\"middle\" x=\"641\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">13 (indem)</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;13 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>17&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M674.0929,-171.7616C668.3574,-160.0176 660.7964,-144.5355 654.3496,-131.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"657.4277,-129.6618 649.8943,-122.2121 651.1377,-132.7337 657.4277,-129.6618\"/>\n",
       "<text text-anchor=\"middle\" x=\"679.3828\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>14</title>\n",
       "<text text-anchor=\"middle\" x=\"725\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">14 (er)</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;14 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>17&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M691.9071,-171.7616C697.6426,-160.0176 705.2036,-144.5355 711.6504,-131.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"714.8623,-132.7337 716.1057,-122.2121 708.5723,-129.6618 714.8623,-132.7337\"/>\n",
       "<text text-anchor=\"middle\" x=\"722.1689\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>15</title>\n",
       "<text text-anchor=\"middle\" x=\"803\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">15 (sich)</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;15 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>17&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M713.8614,-171.8201C722.7663,-166.3143 732.3975,-160.1035 741,-154 752.1235,-146.1078 763.8792,-136.8748 774.1286,-128.5059\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"776.5004,-131.0865 781.9846,-122.0215 772.0444,-125.6879 776.5004,-131.0865\"/>\n",
       "<text text-anchor=\"middle\" x=\"767.9448\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">obj</text>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>16</title>\n",
       "<text text-anchor=\"middle\" x=\"916\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">16 (weiterzugehen)</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;16 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>17&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M732.1109,-171.9883C747.7414,-166.248 765.0998,-159.8649 781,-154 806.1436,-144.7256 833.9084,-134.4485 857.6875,-125.6358\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"859.1118,-128.8406 867.2719,-122.083 856.6788,-122.277 859.1118,-128.8406\"/>\n",
       "<text text-anchor=\"middle\" x=\"838.0518\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">xcomp</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<DependencyGraph with 19 nodes>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import DependencyGraph\n",
    "t = DependencyGraph(tree_string)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 3 (Tagging mit NLTK)\n",
    "\n",
    "Auch NLTK enthält Modelle für Textannotationen. Testen Sie die in den NLTK-Kapiteln 3 und 5 beschriebenen Tagger (**POS, Segmentizer, Stemmer, Lemmatizer**) für das Englische aus (wie man einen POS-Tagger mit NLTK selbst trainiert, um etwa auch auf deutschen Texten POS-Tagging mit NLTK durchzuführen, ist Thema in einer späteren Sitzung).\n",
    "\n",
    "- https://www.nltk.org/book/ch03.html\n",
    "- https://www.nltk.org/book/ch05.html\n",
    "\n",
    "\n",
    "> HINWEIS: NLTK ist nicht primär Annotationstool wie stanza/spacy, sondern eher für Preprocessing und trainieren eigener Modelle geeignet (nur wenige vortrainierte Sprachmodelle in NLTK enthalten).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS-Tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zb: pos-tagging\n",
    "#from nltk.tokenize import word_tokenize\n",
    "text = nltk.word_tokenize(\"And now for something completely different.\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Die', 'NNP'),\n",
       " ('Wahlverwandtschaften', 'NNP'),\n",
       " ('Ein', 'NNP'),\n",
       " ('Roman', 'NNP'),\n",
       " ('von', 'NNP'),\n",
       " ('Johann', 'NNP'),\n",
       " ('Wolfgang', 'NNP'),\n",
       " ('von', 'NNP'),\n",
       " ('Goethe', 'NNP'),\n",
       " ('Erster', 'NNP')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pos_tag basiert auf englischem Modell, kein sinnvolles Ergebnis z.B. für deutsche Texte):\n",
    "text = nltk.word_tokenize(open('../wahlverwandschaften.txt').read())\n",
    "#text[0:10]\n",
    "nltk.pos_tag(text[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('die', 'NN'),\n",
       " ('wahlverwandtschaften', 'WRB'),\n",
       " ('ein', 'JJ'),\n",
       " ('roman', 'NN'),\n",
       " ('von', 'NN'),\n",
       " ('johann', 'NN'),\n",
       " ('wolfgang', 'NN'),\n",
       " ('von', 'IN'),\n",
       " ('goethe', 'NN'),\n",
       " ('erster', 'NN')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Case relevant für POS-Tagging!\n",
    "text = nltk.word_tokenize(open('../wahlverwandschaften.txt').read().lower())\n",
    "#text[0:10]\n",
    "nltk.pos_tag(text[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Segmentation:\n",
    "- https://www.nltk.org/book/ch03.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' And now.', 'Something completely different.']\n"
     ]
    }
   ],
   "source": [
    "raw_text = \" And now. Something completely different.\"\n",
    "\n",
    "sents = nltk.sent_tokenize(raw_text)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK-Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "print([porter.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "print([lancaster.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK-Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DENNIS', ':', 'Listen', ',', 'strange', 'woman', 'lying', 'in', 'pond', 'distributing', 'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "print([wnl.lemmatize(t) for t in tokens])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
